{
 "metadata": {
  "name": "",
  "signature": "sha256:4bb747fd6a397cfc5fd74a86a2f98c4f38ae60596bd34dafe443c17fcc1f47c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem set 3: Parsing\n",
      "===========\n",
      "\n",
      "This problem set contains two parts. \n",
      "\n",
      "- Grammar design: try to design a CFG to parse sentences on Twitter.\n",
      "- Dependency parsing: design features to learn a high-accuracy dependency parser\n",
      "\n",
      "### Honor policy ###\n",
      "\n",
      "- Your work must be your own. Do not discuss the details of the assignment with other people. \n",
      "- You may of course help each other with understanding the ideas discussed in lecture and the readings, and with basic questions about programming in Python. For example, it is fine to discuss how arc-factored dependency parsing works, or how state-splitting works in CFGs. It is **not acceptable** to discuss how to implement a specific feature for dependency parsing, or how to handle a specific linguistic phenomenon in a CFG. It is unacceptable to share your code.\n",
      "- There are implementations and source code for many machine learning algorithms on the internet. Please write the code for this assignment on your own, without using these external resources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parseTwitter import evalParser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: CFG Design #\n",
      "\n",
      "Your first job is to design a CFG to parse Twitter. We provide some files:\n",
      "\n",
      "- **oct27.clean.train** contains some short sentences from the Twitter POS tagging dataset, avoiding some difficult tags like URLs and retweets.\n",
      "- **parseTwitter.py** contains some code for running a CFG parser on Twitter, using nltk\n",
      "- **simple.cfg** is a minimal CFG, using some ideas we've seen in class\n",
      "\n",
      "The setup is as follows:\n",
      "\n",
      "- You will see input with real sentences from Twitter, as well as scrambled sentences\n",
      "- The terminal symbols are tags, not words. See [here](http://www.ark.cs.cmu.edu/TweetNLP/annot_guidelines.pdf) for more on the tagnset.\n",
      "\n",
      "Scoring:\n",
      "\n",
      "- If your grammar produces at least one parse for a real sentence, that's a true positive; otherwise it's a false negative\n",
      "- If your grammar produces at least one parse for a scrambled sentence, that's a false negative\n",
      "- Your goal is to get a high f-measure, while producing as few parses per sentence as possible\n",
      "- My minimal grammar gets an F-measure of 9.8% (not very good!), with 1.6 parses per parsed sentence.\n",
      "- evalParser takes an optional argument \"show_fns=True\", which will show you the sentences that you are not successfully parsing, and an optional argument \"show_fps=True\", which will show scrambled sentences that were successfully parsed.\n",
      "\n",
      "We will run a bakeoff to see who can get the highest score on each deliverable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:simple.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "{'f-measure': 0.09803908688980874,\n",
        " 'parses-per-sent': 1.5999968000064002,\n",
        " 'precision': 0.7142856122449125,\n",
        " 'recall': 0.05263157894736842}"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 1 ** (5 points): design a grammar that gets an f-measure of at least 0.3. Name it \"lastname-firstname.all.cfg\", submit it with your notebook on T-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:anand-arjun.all.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "{'f-measure': 0.4253388744298359,\n",
        " 'parses-per-sent': 7.170211240380587,\n",
        " 'precision': 0.37301587005542963,\n",
        " 'recall': 0.49473684210526314}"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 2 ** (5 points): design a grammar that gets an f-measure of at least 0.25, with no more than 10 parses per sentence. If your previous grammar already got fewer than 10 parses per sentence, then change it to reduce the number of parses per sentence even further. Name it \"lastname-firstname.10.cfg\", and submit it on t-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:anand-arjun.10.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "{'f-measure': 0.4131450437970164,\n",
        " 'parses-per-sent': 5.136362469008529,\n",
        " 'precision': 0.3728813527721919,\n",
        " 'recall': 0.4631578947368421}"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Splitting tags (7650 only) ##\n",
      "\n",
      "The Twitter POS tagset is very coarse-grained. You might be able to make a better parser if you could distinguish between subcategories of tags. In this problem, you will try to do this.\n",
      "\n",
      "- The evalParser() code takes an optional argument \"preprocessor\", which you can use to modify the tags in a sentence.\n",
      "- Your parser can then add special handling for the modified tags.\n",
      "\n",
      "For example, the function below splits the \"P\" tag to have a special tag \"2\", for the word \"to\". The grammar jacob.split.cfg can then use the tag \"2\" as a terminal symbol, slightly improving the f-measure and reducing the number of parses per sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def myPreprocess(words,tags):\n",
      "    for i,word in enumerate(words):\n",
      "        if (word.lower() == 'to'):\n",
      "            tags[i] = '2'\n",
      "        \n",
      "        if i>1 and i<(len(words)-1):\n",
      "            if tags[i-1] == 'N' and tags[i+1] == 'N':\n",
      "                tags[i] = 'AND'\n",
      "            \n",
      "            \n",
      "    return words,tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser('file:anand-arjun.split.cfg',preprocessor=myPreprocess)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'f-measure': 0.3902434032130077,\n",
        " 'parses-per-sent': 3.8999990250002434,\n",
        " 'precision': 0.36363636033057856,\n",
        " 'recall': 0.42105263157894735}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3** (3 points; 7650 only): use tag splitting to improve your parser. \n",
      "\n",
      "- A small improvement like the one I obtained above is sufficient, but use a different split than \"to\". \n",
      "- You may check out [Klein and Manning 2003](nlp.stanford.edu/pubs/unlexicalized-parsing.pdf) for ideas. \n",
      "- Name your grammar \"lastname-firstname.split.cfg\", and submit it on T-square."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2: Dependency parsing #\n",
      "\n",
      "In this problem, you will work with an arc-factored non-projective dependency parser, which is trained by average perceptron. If you were not confident about your own implementation of averaged structure perceptron, please take a look at the code, in the directories shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('parsing/')\n",
      "\n",
      "import dependency_parser as depp\n",
      "import dependency_features as depf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build a dependency parser with a given feature set\n",
      "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
      "dp.read_data(\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing\\..\\data\\deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 801"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train your parser for ten iterations\n",
      "# These are *unlabeled* accuracies.\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.477\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.502\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.503\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.504\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.508\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.509\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.511\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You are going to create a series of subclasses of DependencyFeatures, which has features of your choice. Here is an example, which adds a feature that includes:\n",
      "\n",
      "- The part-of-speech of the head\n",
      "- The word of the modifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexFeats(depf.DependencyFeatures):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        f = self.getF((k,instance.pos[h],instance.words[m]),add)\n",
      "        ff.append(f)\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several things to notice about this code.\n",
      "\n",
      "- You start by calling the same function, using the parent class. You can build a chain of feature functions in this way.\n",
      "- $h$ provides the index of the head word of the dependency arc\n",
      "- $m$ provides the index of the modifier word of the dependency arc\n",
      "- You can access the part of speech tags in the instance as instance.pos[i], where i indexes any word token\n",
      "- You can access the words themselves as instance.words[i], where $i$ again indexes the token\n",
      "- To create a feature, you call getF(), with two arguments:\n",
      "    - A feature tuple, which includes an index $k$, and any other information you want -- it need not be a tuple of exactly three items\n",
      "    - An argument \"add\", which you don't need to worry about (but you do need to include)\n",
      "    - Make sure to keep $k$ up-to-date. This prevents collisions in the space of features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's run it\n",
      "dp = depp.DependencyParser(feature_function=LexFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23019"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.535 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.550\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.585 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.565\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.602 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.615 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.625 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.631 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.638 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.640 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.642 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.646 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4 ** (3 points): start by adding a feature that computes the distance between the head and the modifier, up to a maximum absolute value of 10.\n",
      "\n",
      "**Sanity check**: you should now have 23039 features. Accuracy should improve substantially."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import sign"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats(LexFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexDistFeats,self).create_arc_features(instance,h,m,add)\n",
      "        dist = h-m\n",
      "        dist *= sign(dist)\n",
      "        if dist <= 10: \n",
      "            k = len(ff)    \n",
      "            f = self.getF((k,dist), add) # your code here\n",
      "            ff.append(f)\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing\\..\\data\\deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23029"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.667 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.730\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.708 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.733\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.728 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.722\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.742 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.720\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.752 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.703\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.759 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.704\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.764 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.711\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.771 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.710\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.774 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.704\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.776 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.708\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 5 ** (3 points): now add a feature to LexDistFeats, which includes the POS of the modifier and the word of the head.\n",
      "\n",
      "**Sanity check**: The number of features should roughly double. (Do you see why?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats2(LexDistFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexDistFeats2,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)    \n",
      "        f = self.getF((k,instance.pos[m],instance.words[h]),add)\n",
      "        ff.append(f)\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats2())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing\\..\\data\\deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 44767"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.688 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.740\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.757 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.752\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.785 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.762\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.806 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.767\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.822 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.766\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Context features ## \n",
      "\n",
      "Add context features that consider the tags adjacent to the head and modifier. You may wish to consider various tag combinations, such as \n",
      "\n",
      " - $\\langle t[h], t[h-1], t[m]\\rangle$: head, head-left, modifier\n",
      " - $\\langle t[h], t[m], t[m+1]\\rangle$: head, modifier, modifier-right\n",
      " - $\\langle t[h], t[h-1], t[m], t[m+1]\\rangle$: head, head-left, modifier, modifier-right\n",
      " - etc\n",
      "\n",
      "Note that you can add more than one feature at a time within create_arc_features(). Watch out for edge cases!\n",
      "\n",
      "** Deliverable 6 ** (5 points):\n",
      "\n",
      "Describe what context feature templates you have added. How do they impact the total number of features? How does \n",
      "it impact the development and training accuracy?\n",
      "\n",
      "** Sanity check **: I added a few basic context features, and this increased dev set accuracy above 80%."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dependency_reader as depr\n",
      "from dependency_reader import * \n",
      "\n",
      "class ContextFeats(LexDistFeats2):\n",
      "    def __init__(self):\n",
      "        super(ContextFeats,self).__init__()\n",
      "        self.reader = DependencyReader()\n",
      "        self.reader.load(\"english\")\n",
      "        self.word_dict = dict((v,k) for k, v in self.reader.word_dict.iteritems())\n",
      "        self.pos_dict = dict((v,k) for k, v in self.reader.pos_dict.iteritems())\n",
      "\n",
      "    def get_root_mod_index(self, instance):\n",
      "        for i in range(1,len(instance.words)):\n",
      "            if instance.heads[i] == 0:\n",
      "                return i\n",
      "        \n",
      "        return -1\n",
      "    \n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(ContextFeats,self).create_arc_features(instance,h,m,add)\n",
      "        if h > 1:\n",
      "            f = self.getF((len(ff),instance.pos[h],instance.pos[h-1],instance.pos[m]),add)\n",
      "            ff.append(f)\n",
      "        if m < (len(instance.words)-1):\n",
      "            f = self.getF((len(ff),instance.pos[h],instance.pos[m],instance.pos[m+1]),add)\n",
      "            ff.append(f)\n",
      "        if h>1 and m < (len(instance.words)-1):\n",
      "            f = self.getF((len(ff),instance.pos[h],instance.pos[h-1],instance.pos[m],instance.pos[m+1]),add)\n",
      "            ff.append(f)\n",
      "            \n",
      "        root_mod = self.get_root_mod_index(instance)\n",
      "        if root_mod != -1:\n",
      "            f = self.getF((len(ff),(h-root_mod), (m-root_mod)),add)\n",
      "            ff.append(f)\n",
      "        \n",
      "        if (instance.words[h] in self.word_dict) and (instance.words[m] in self.word_dict):\n",
      "            f = self.getF((len(ff),len(self.word_dict[instance.words[h]]), len(self.word_dict[instance.words[m]])),add)\n",
      "            ff.append(f)\n",
      "        \n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "I have added two new context features not mentioned above. They are - \n",
      "1) Distance from root modifier - head and modifier\n",
      "2) Length of head and modifier words\n",
      "\n",
      "This adds 2 additional features per arc. So it approximately increases the total number of features by 40%\n",
      "These features have a significant impact on the accuracies. \n",
      "\n",
      "Feature 1  is important because it gives a sense of what length \n",
      "an arc can span across a given sentence w.r.t the root modifier. This helps establish a relative context for each arc to the root.\n",
      "\n",
      "Feature 2 is more of a general feature which establishes a sense of the relationship between the head and modifier for a given arc\n",
      "in a linguistic context."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=ContextFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10) #this needs to be run again"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing\\..\\data\\deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Done with init!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "parsing\\..\\data\\deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 66293"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.800 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.820\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.863 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.843\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.883 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.847\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.900 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.858\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.911 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.862\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.919 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.865\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.927 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.866\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.932 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.868\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.937 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.867\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.943 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.868\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bakeoff! ##\n",
      "\n",
      "Try to develop other features that will further improve performance.\n",
      "Please explain all the features that you have. \n",
      "After identifying your best feature set, run *test()* function from DependencyParser. This will produce a file **data/deppars/english_test.conll.pred**.\n",
      "\n",
      "**Deliverable 7** (5 points): Rename this to **lastname-firstname.conll.pred** and include it in your t-square submission.\n",
      "\n",
      "**Sanity check / challenge**: The best test set performance in Fall 2013 was 87.2%. Can you beat it??\n",
      "\n",
      "**NOTE**: As usual, you are not supposed to look at other code online while doing this problem set. However, you are welcome to search for *research papers* on dependency parsing to get ideas for features that might improve your performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "dp.test() \n",
      "#please run this after training the perceptron from Deliverable 6. My computer proved too slow for me to be able to do this in time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}