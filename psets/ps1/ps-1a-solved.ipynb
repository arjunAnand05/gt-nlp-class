{
 "metadata": {
  "name": "",
  "signature": "sha256:e131c66bc537c7a01f29619980cd4d35f46151fd594905ef8dbe8ea62e8a5c48"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 1a: Text classification\n",
      "==============\n",
      "\n",
      "(_This problem set is graded out of 30 for students taking CS4650 and out of 35 for students taking CS7650. But like all problem sets, this will count towards 6% of your final grade _)\n",
      "\n",
      "\n",
      "In this problem set, you will build a system for classifying movie reviews as positive, negative, or neutral. You will:\n",
      "\n",
      "- Do some basic text processing, tokenizing your input and converting it into a bag-of-words representation\n",
      "- Build a classifier based on sentiment word lists\n",
      "- Build a machine learning classifier, using Naive Bayes\n",
      "- Evaluate your classifiers and examine what they have learned\n",
      "\n",
      "In Problem Set 1b (due one week later), you will try some more advanced classifiers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Installation ##\n",
      "\n",
      "You may need to install some of the libraries below. Usually this is done with pip or easy_install. See here:\n",
      "\n",
      "- [NLTK](http://www.nltk.org/install.html)\n",
      "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
      "- [numpy](http://docs.scipy.org/doc/numpy/user/install.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize, sent_tokenize,RegexpTokenizer\n",
      "from collections import defaultdict, Counter\n",
      "from matplotlib import pyplot as plt\n",
      "import numpy as np\n",
      "import scorer\n",
      "import operator\n",
      "import math\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "(_Completing docsToBOWs() - 4 pts, each question in Deliverable 1 is worth 1 pt. Total 8 pts for part 1_)\n",
      "\n",
      "Your first step is to write code that can apply the following\n",
      "preprocessing steps. You will have to run this code fairly quickly on\n",
      "the test data when you receive it, so make sure it is modular and\n",
      "well-written.\n",
      "\n",
      "- You will edit a function that takes as its argument a \"key\" document.\n",
      "  It should produce a \"BOW\" (bag-of-words) document.\n",
      "  Each line of the key document contains a filename and a label.\n",
      "  Each line of the BOW document should contain a BOW representation of the corresponding\n",
      "  file in the key document. \n",
      "- A BOW representation looks like this: \"word:count word:count word:count...\" for every word that appears in\n",
      "  the document. Do not print words that have zero count. Use space delimiters.\n",
      "- Use NLTK's [tokenization package](http://nltk.org/api/nltk.tokenize.html) function \n",
      "  to divide each file into sentences, and each sentence into tokens.\n",
      "- Downcase all tokens\n",
      "- Only consider tokens that are completely alphabetic.\n",
      "\n",
      "I have provided some shell code, but you will have to fill in the tokenization and filtering steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "wnl = WordNetLemmatizer()\n",
      "stops = set(stopwords.words('english'))\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "def populateDict(sentences, dict):\n",
      "    for sentence in sentences:\n",
      "        words = tokenizer.tokenize(sentence)\n",
      "        for word in words:\n",
      "            if (word in stops) or re.search('[0-9]+',word):\n",
      "                continue\n",
      "            word  = word.lower()\n",
      "            if word in dict:\n",
      "                dict[word] += 1\n",
      "            else:\n",
      "                dict[word] = 1\n",
      "\n",
      "    return dict\n",
      "\n",
      "def docsToBOWs(keyfile):\n",
      "    with open(keyfile,'r') as keys:\n",
      "        with open(keyfile.replace('.key','.bow'),'w') as outfile:\n",
      "            for keyline in keys:\n",
      "                dataloc = keyline.rstrip().split(' ')[0]\n",
      "                fcounts = dict([])\n",
      "                with open(dataloc,'r') as infile:\n",
      "                    for line in infile:\n",
      "                        populateDict(sent_tokenize(line), fcounts)\n",
      "                for word,count in fcounts.items():\n",
      "                    print >>outfile,\"{}:{}\".format(word,count), #write the word and its count to a line\n",
      "                print >>outfile,\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the keyfiles that are relevant to this homework. \n",
      "At the beginning you won't have test-imdb.key"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainkey = 'train-imdb.key'\n",
      "devkey = 'dev-imdb.key'\n",
      "testkey = 'test-imdb.key'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, run these lines to produce the BOW files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docsToBOWs(trainkey)\n",
      "docsToBOWs(devkey)\n",
      "docsToBOWs('test-imdb.key') # you won't have this file yet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next cell defines a [generator function](http://wiki.python.org/moin/Generators), called \"dataIterator\"\n",
      "\n",
      "- This allows you to easily iterate through the dataset defined by a given keyfile. \n",
      "- Each time you call \"next\" (possibly implicitly), it returns a dict containing features and counts for the next document in the sequence. \n",
      "- In this case, the features include the words, and a special \"offset\" feature\n",
      "- This is equivalent to $\\boldsymbol{x}_i$ in the reading.\n",
      "- You can see how this is used in the getAllCounts() function below, which takes a dataIterator as an argument.\n",
      "\n",
      "Lines 7-8 of the code might look confusing if you are not a pythonista. \n",
      "\n",
      "- This is a [list comprehension](http://legacy.python.org/dev/peps/pep-0202/)\n",
      "nested inside a [dict comprehension](http://legacy.python.org/dev/peps/pep-0274/).\n",
      "- Here's an [introduction](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html) with more examples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "offset = '**OFFSET**'\n",
      "def dataIterator(keyfile):\n",
      "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
      "        with open(keyfile,'r') as keys:\n",
      "            for keyline in keys:\n",
      "                textloc,label = keyline.rstrip().split(' ')\n",
      "                fcounts = {word:int(count) for word,count in\\\n",
      "                           [x.split(':') for x in bows.readline().rstrip().split(' ')]}\n",
      "                fcounts[offset] = 1\n",
      "                yield fcounts,label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataIterator above incrementally re-reads the keyfile and BOW file every time you call it. \n",
      "This is a good idea if you have huge data that won't fit in memory, but the file I/O involves some overhead.\n",
      "If you want, you can write a second dataIterator that iterates across data stored in memory, which\n",
      "will be faster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: How many unique words appear in the training set? (Types, not tokens.) I get 24861. (Don't count the offset feature.)\n",
      "\n",
      "- Note how the dataIterator function is used here. \n",
      "- fcounts is a dict, which it returns for each document.\n",
      "- We are currently ignoring the label, but that is also provided.\n",
      "- This may take a couple of minutes to run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getAllCounts(datait):\n",
      "    allcounts = Counter()\n",
      "    for fcounts, _ in datait:\n",
      "        allcounts += Counter(fcounts)\n",
      "    return allcounts\n",
      "\n",
      "ac_train = getAllCounts(dataIterator('train-imdb.key'))\n",
      "ac_dev = getAllCounts(dataIterator('dev-imdb.key'))\n",
      "ac_test = getAllCounts(dataIterator('test-imdb.key'))\n",
      "print \"number of word types\",len(ac_train.keys())-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of word types 23805\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code makes a plot, with the log-rank (from 1 to the log of the total number of words) \n",
      "on the x-axis and the log count on the y-axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this enables you to create inline plots in the notebook \n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_logcounts = np.log(np.array(sorted(ac_train.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(tr_logcounts))),tr_logcounts)\n",
      "dv_logcounts = np.log(np.array(sorted(ac_dev.values(),reverse=True)))\n",
      "plt.plot(np.log(range(len(dv_logcounts))),dv_logcounts,'r')\n",
      "plt.xlabel('log rank')\n",
      "plt.ylabel('log count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: RuntimeWarning: divide by zero encountered in log\n",
        "-c:4: RuntimeWarning: divide by zero encountered in log\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<matplotlib.text.Text at 0xb9f6470>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xba15710>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1**\n",
      "\n",
      "- Explain what you see in the plot. Does it observe Zipf's law? How well?\n",
      "- Print the token/type ratio for both the training and dev data.\n",
      "- Print the number of types which appear exactly once in the training and dev data\n",
      "- Print the number of types that appear in the dev data but not the training data (hint: use [sets](https://docs.python.org/2/library/sets.html) for this)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(add cells with your answer)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printTokenTypeRatio(keyfile):\n",
      "    wordTypes = set([])\n",
      "    singleWordTypes = set([])\n",
      "    tokenCount = 0\n",
      "    with open(keyfile.replace('key','bow'),'r') as keys:\n",
      "        for keyline in keys:\n",
      "            wordset = keyline.rstrip().split(' ')\n",
      "            for wordcount in wordset:\n",
      "                word,frequency = wordcount.split(':')\n",
      "                wordTypes.add(word)\n",
      "                tokenCount += int(frequency)\n",
      "                if int(frequency) == 1 :\n",
      "                    singleWordTypes.add(word)\n",
      "    print \"For \",keyfile,\" Token count =\",tokenCount,\" type count = \",len(wordTypes),\" and single word count = \",len(singleWordTypes)\n",
      "    return wordTypes\n",
      "\n",
      "singleTrainWords = printTokenTypeRatio(trainkey)\n",
      "singleDevWords = printTokenTypeRatio(devkey)\n",
      "print \"Words in dev, but not in training = \",len(singleDevWords.difference(singleTrainWords))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " For  train-imdb.key  Token count = 487655  type count =  25858  and single word count =  24682\n",
        "For "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev-imdb.key  Token count = 233993  type count =  17554  and single word count =  16584\n",
        "Words in dev, but not in training =  5008\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "The plot shows the relationship between the log values of the rank and frequency for the given sets i.e. training\n",
      "and dev. Zipfs law is not strictly obeyed here because the lines are not linear entirely. However, there is \n",
      "a significant portion of the graph where Zipfs law is obeyed and the rank is directly proportional to the inverse of the \n",
      "frequency. This can be observed in the region between(2,8) and (8,2). This means that for words with ranks and frequencies\n",
      "between e^2 and e^8, their ranks will be between e^8 and e^2(inversely related)."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "For training data, tokens  = 487655 and types  = 25858. Therefore, ratio = 0.053\n",
      "For dev data, tokens  = 233993 and types = 17554. Therefore, ratio = 0.07501"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "24682 words appear exactly once in the training data\n",
      "16584 words appear exactly once in the dev data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Words in dev data, but not in training data = 5008"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. Word Lists #\n",
      "(_Completing predict() - 3 pts, setting weights - 2 pts, Deliverable 2 - 1 pt. Total 7 pts for part 2_)\n",
      "\n",
      "- We will now build a sentiment analysis system based on word lists. \n",
      "- The file \"sentiment-vocab.tff\" contains a sentiment lexicon from [ Wilson et al 2005](http://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf). \n",
      "- The code below reads the lexicon into memory, building sets of positive and negative words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poswords = set()\n",
      "negwords = set()\n",
      "neuwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for i,line in enumerate(fin):\n",
      "        # more list and dict comprehensions!\n",
      "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
      "        if kvs['type'] == 'strongsubj':\n",
      "            if kvs['priorpolarity'] == 'negative':\n",
      "                negwords.add(kvs['word1'])\n",
      "            elif kvs['priorpolarity'] == 'positive':\n",
      "                poswords.add(kvs['word1'])\n",
      "            else :\n",
      "                neuwords.add(kvs['word1'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, you should write a classifier that classifies each instance in a testfile. The classification rule is:\n",
      "\n",
      "- 'POS' if the instance has more words from the positive list than the negative list\n",
      "- 'NEG' if the instance has more words from the negative list than the positive list\n",
      "- 'NEU' (neutral) if the instance has the same number of words from each list\n",
      "\n",
      "To do this, you will write a function \"predict\", \n",
      "which represents the inner-product computation $\\boldsymbol{\\theta}' \\boldsymbol{f}(\\boldsymbol{x},y)$.\n",
      "It should have the following characteristics:\n",
      "\n",
      "- **Input 1** an instance, represented as a dict (with features as keys and counts as values) \n",
      "- **Input 2** a dictionary of weights, where keys are tuples of features and labels, and weights are the values. This corresponds to $\\boldsymbol{\\theta}$ in the reading. See example below.\n",
      "- **Input 3** a list of possible labels\n",
      "- **Output 1** the highest-scoring label\n",
      "- **Output 2** a dict with labels as keys and scores as values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(instance,weights,labels):\n",
      "    scores = dict([('POS',0),('NEU',0),('NEG',0)])\n",
      "    for word,count in instance.iteritems():\n",
      "        for label in labels:\n",
      "            scores[label] += weights[(label,word)] * count\n",
      "    argmax = lambda scores : max(scores.iteritems(),key=operator.itemgetter(1))[0]\n",
      "    return argmax(scores),scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are weights for the simplest classifier, which simply labels all instances as positive.\n",
      "\n",
      "Note that it uses only the 'offset' feature"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_labels = ['POS','NEG','NEU']\n",
      "weights_all_pos = defaultdict(int)\n",
      "weights_all_pos.update({('POS',offset):1,('NEG',offset):0,('NEU',offset):0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is some code for evaluating your classifiers. \n",
      "It uses a scoring library that I wrote, and writes the output to a file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalClassifier(weights,outfilename,testfile=devkey):    \n",
      "    with open(outfilename,'w') as outfile: #open the output file\n",
      "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
      "            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below shows how to evaluate this classifier. \n",
      "\n",
      "- **Sanity check**: You should get 40.7% accuracy just by classifying everything as positive. This is the \"most common class\" (MCC) baseline.\n",
      "\n",
      "The printed output is a **confusion matrix**. \n",
      "The rows indicate the key and the columns indicate the response. \n",
      "In this case, the response is always \"POS\", so there is only one column. \n",
      "The cell NEG/POS tells you how often an example that was labeled \"NEG\" in the key was labeled \"POS\" in the system response."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_all_pos,'all_pos.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "1 classes in response: set(['POS'])\n",
        "confusion matrix\n",
        "key\\response:\tPOS\n",
        "NEG\t\t401\t\n",
        "NEU\t\t192\t\n",
        "POS\t\t407\t\n",
        "----------------\n",
        "accuracy: 0.4070 = 407/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now build a classifier based on the word lists. The classifier should have the following decision rule:\n",
      "\n",
      "- If the number of positive words (tokens) is greater than the number of negative words, choose label 'POS'\n",
      "- If the number of negative words (tokens) is greater than the number of positive words, choose label 'NEG'\n",
      "- If they are equal, choose label 'NEU'\n",
      "\n",
      "You should manually set the weights to ensure this behavior; don't change the predict function at all.\n",
      "You'll need to use the offset weights to make sure that ties go to the 'NEU' label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "weights_list = dict({('POS',offset):0,('NEG',offset):0,('NEU',offset):0})\n",
      "for word in poswords:\n",
      "    weights_list[('POS',word)] = 1\n",
      "    weights_list[('NEG',word)] = 0\n",
      "    weights_list[('NEU',word)] = 0\n",
      "for word in negwords:\n",
      "    weights_list[('POS',word)] = 0\n",
      "    weights_list[('NEG',word)] = 1\n",
      "    weights_list[('NEU',word)] = 0\n",
      "for word in neuwords:\n",
      "    weights_list[('POS',word)] = 0\n",
      "    weights_list[('NEG',word)] = 0\n",
      "    weights_list[('NEU',word)] = 1\n",
      "    \n",
      "#argmax = lambda x : 'POS' if (x['POS'] > x['NEG']) else 'NEG' if (x['NEG'] > x['POS']) else 'NEU' \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2**: run your classifier on dev.key, and use the following code to print the resulting confusion matrix.\n",
      "\n",
      "The confusion matrix should now have three columns, since the response should include every class at least once. The count of correct responses is found on the diagonal of the confusion matrix. What is the most frequent type of error?\n",
      "\n",
      "**Sanity check**: The accuracy should be 55.9%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mat = evalClassifier(weights_list,'word_list.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
        "confusion matrix\n",
        "key\\response:\tNEG\tNEU\tPOS\n",
        "NEG\t\t227\t17\t157\t\n",
        "NEU\t\t61\t7\t124\t\n",
        "POS\t\t60\t19\t328\t\n",
        "----------------\n",
        "accuracy: 0.5620 = 562/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No.of words in positive lexicon = 1482\n",
      "No.of words in negative lexicon = 3079\n",
      "No.of words in neutral lexicon = 191\n",
      "\n",
      "The most frequent type of error is the classification of words in the negative lexicon, as positive. \n",
      "This is most likely because the size of the negative lexicon is much larger(3079 words as opposed to 1482 in positive). As a result, the likelihood of error is higher for the negative lexicon words. \n",
      "Also, the weight of the negative words might be more than the weight of the positive words, but since we're only taking frequency into account, the number of occurences of positive words might still be more. This way, a sentence can be incorrectly classified as postive despite being more negative in nature."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. Naive Bayes #\n",
      "(_Completing learnNBWeights() - 5 pts, Deliverable 3a - 1pt, 3b - 1 pt, explanation of plot output - 2pts. Total 8 points for part 3_)\n",
      "\n",
      "Now you will implement a Naive Bayes classifier.\n",
      "\n",
      "You already have the code for the decision function, \"predict\". \n",
      "So you just need to construct a set of weights that correspond to the classifier. \n",
      "These weights will contain two parameters:\n",
      "\n",
      "- $\\log \\mu$ for the offset, which parametrizes the prior $\\log P(y)$\n",
      "- $\\log \\phi$ for the word counts, which parametrizes the likelihood $\\log P(x | y)$\n",
      "\n",
      "You should use maximum *a posteriori* estimation of\n",
      "the parameter $\\phi$,\n",
      "$$\\phi_{j,n} = P(w = n | y = j) = \\frac{\\sum_{i: y_i = j} x_{i,n} + \\alpha}{\\sum_{i:y_i=j} \\sum_{n'} x_{i,n'} + V\\alpha}$$\n",
      "where \n",
      "\n",
      "- $y_i = j$ indicates the class label $j$ for instance $i$\n",
      "- $w=n$ indicates word $n$\n",
      "- $\\alpha$ is the smoothing parameter\n",
      "- $V$ is the total number of words\n",
      "\n",
      "For each class, normalize by the sum of counts of words **in that class**. In other words, $\\sum_j \\phi_{j,n} = 1$ for all $j$. You can estimate $\\log \\phi$ directly if you prefer.\n",
      "\n",
      "For the prior $\\log P(y)$, you can use relative frequency estimation.\n",
      "\n",
      "Both probabilities should be estimated from the training data only.\n",
      "Please write this code yourself -- do not use other libraries, and try to do\n",
      "it without looking at other code online."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain #hint, especially if you're obsessive about being pythonic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the word counts first, because this can be slow\n",
      "# you may also wish to keep a list of all word types that are observed in the training data\n",
      "counts = defaultdict(lambda : Counter()) # hint\n",
      "class_counts = defaultdict(int) # hint\n",
      "word_types = set()\n",
      "total_words = 0\n",
      "for fcounts,label in dataIterator(trainkey):\n",
      "    class_counts[label] += 1\n",
      "    counts[label] += Counter(fcounts)\n",
      "    word_types |= set(fcounts.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should write a *function* to compute the weights for a given value of $\\alpha$, \n",
      "because you will want to vary this value later.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learnNBWeights(alpha=0.1):\n",
      "    labels = set(['POS','NEG','NEU'])\n",
      "    total_docs = sum(class_counts.values())\n",
      "    weights = defaultdict(float)\n",
      "    weights[('POS',offset)] = math.log(float(class_counts['POS'])/total_docs)\n",
      "    weights[('NEG',offset)] = math.log(float(class_counts['NEG'])/total_docs)\n",
      "    weights[('NEU',offset)] = math.log(float(class_counts['NEU'])/total_docs)\n",
      "    for word in word_types:\n",
      "        for label in labels:\n",
      "            weights[label,word] = math.log(float(float(counts[label][word] + alpha)/float(sum(counts[label].values()) + len(word_types)*alpha)))\n",
      "    return weights\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3a**\n",
      "Train a classifier from the training data, and apply it to\n",
      "the development data, with $\\alpha = 0.1$. Report the confusion matrix and the accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the weights\n",
      "weights_nb = learnNBWeights(alpha=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: For the following instance, I get the scores shown."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label, scores = predict({'good':1,'worst':4,offset:1},weights_nb,all_labels)\n",
      "print label , scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NEG {'NEG': -36.221678317666445, 'NEU': -41.427445298831714, 'POS': -45.46521905341461}\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to evaluate your weights\n",
      "mat = evalClassifier(weights_nb,'nb.txt')\n",
      "print scorer.printScoreMessage(mat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
        "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
        "confusion matrix\n",
        "key\\response:\tNEG\tNEU\tPOS\n",
        "NEG\t\t331\t26\t44\t\n",
        "NEU\t\t92\t29\t71\t\n",
        "POS\t\t64\t46\t297\t\n",
        "----------------\n",
        "accuracy: 0.6570 = 657/1000\n",
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** Try at least seven different values of $\\alpha$. Plot the accuracy on both the dev and training sets for each value, using [subplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) to show two plots in the same cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr_accs = []\n",
      "dv_accs = []\n",
      "alphas = [0.1,1,2.7,6.3,9,14,18,22,32,100]# your choice\n",
      "weights_nb_alphas = dict()\n",
      "for alpha in alphas:\n",
      "    print alpha,\n",
      "    # learn the weights\n",
      "    weights_nb_alphas[alpha] = learnNBWeights(alpha) \n",
      "    # evaluate on training data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.tr.txt',trainkey)\n",
      "    tr_accs.append(scorer.accuracy(confusion))\n",
      "    # evaluate on dev data\n",
      "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.dv.txt',devkey)\n",
      "    dv_accs.append(scorer.accuracy(confusion))\n",
      "    print dv_accs[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.654\n",
        "1.1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.676\n",
        "2.7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.68\n",
        "6.3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.688\n",
        "9 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.685\n",
        "14 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.668\n",
        "18 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.663\n",
        "22 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.656\n",
        "32 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.64\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this code to plot the accuracies\n",
      "subplot(1,2,1)\n",
      "plot(log(alphas),tr_accs,'bx-')\n",
      "ylabel('training accuracy')\n",
      "subplot(1,2,2)\n",
      "plot(log(alphas),dv_accs,'rx-')\n",
      "ylabel('dev accuracy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "<matplotlib.text.Text at 0x12ed56d8>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xba9b550>"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plot shows that the accuracy of a classifier varies non-linearly with the value of alpha. There is typically a value of alpha for which the accuracy peaks and then drops again. Therefore, alpha must be varied randomly to find the value for which the accuracy is the highest.\n",
      "If we use a larger sample space for alpha, we will observe this pattern even in the training data set.\n",
      "Also, the optimal value of alpha differs for different data sets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Feature Analysis #\n",
      "\n",
      "(_Completing  getTopFeats() - 2 pts, Deliverable 4a - 1pt, 4b - 2 pts, 4c - 2 pts, 4d -5pts . Total 7 pts for CS4650 and 12 pts for CS7650_)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a**\n",
      "What are the words that are most predictive of positive versus negative text?\n",
      "You can measure this by $\\log \\theta_{pos,n} - \\log \\theta_{neg,n}$ (which is similar to the [likelihood ratio test](http://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
      "Use $\\alpha = 1$ from the dev data.\n",
      "\n",
      "List the top five words and their counts for each class. Do the same for the top 5 words that predict negative versus positive.\n",
      "\n",
      "Consider using [operator.itemgetter()](http://docs.python.org/2.7/library/operator.html) for easily sorting dictionaries by their values. See my definition of the argmax function above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getTopFeats(weights,class1,class2,K=5):\n",
      "    diff_vals = defaultdict(int)\n",
      "    for word in word_types:\n",
      "        diff_vals[word] = weights[class1,word] - weights[class2,word]\n",
      "    strong_words = dict(sorted(diff_vals.iteritems(), key=operator.itemgetter(1), reverse=True)[:K])\n",
      "    word_counts = dict()\n",
      "    for word in strong_words:\n",
      "        word_counts[word] = counts[class1][word],counts[class2][word]\n",
      "        \n",
      "    return word_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "print getTopFeats(weights_nb_alphas[1],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[1],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'yokai': (19, 0), 'friendship': (22, 0), 'tarzan': (20, 0), 'spock': (18, 0), 'palma': (17, 0)}\n",
        "{'awful': (107, 4), 'kibbutz': (17, 0), 'carlito': (19, 0), 'insult': (19, 0), 'sellers': (18, 0)}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** Now do the same thing for $\\alpha = 100$. Which words look better to you? \n",
      "Which gave better accuracy? \n",
      "Explain what you think is going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "print getTopFeats(weights_nb_alphas[100],'POS','NEG')\n",
      "print getTopFeats(weights_nb_alphas[100],'NEG','POS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'great': (464, 135), 'wonderful': (103, 13), 'love': (294, 126), 'best': (318, 110), 'excellent': (117, 19)}\n",
        "{'awful': (107, 4), 'bad': (513, 128), 'waste': (82, 7), 'worst': (173, 19), 'worse': (115, 17)}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The words for alpha = 100 look much better than those for alpha=1. This shows that alpha=100 gives a much better accuracy than alpha=1\n",
      "This is because setting alpha to 100 puts the weights for various words into the perspective of their frequencies in the given dataset. As a result, words that appeared in both lexicons were assigned a better and stronger weight as they indicate a stronger sense of opinion. \n",
      "With alpha=1, words that entirely appeared only in the negative lexicon were given more weight, which is not necessarily the right way of classification, as shown above. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** Consider the weights for $\\alpha=100$. \n",
      "\n",
      "- Print words $w$ that are in the positive lexicon, but for which $\\log \\phi_{neg,w} > \\log \\phi_{pos,w} + 0.1$. \n",
      "    - (These words are more likely in the negative class, despite being in the positive lexicon.)\n",
      "- Print words $w$ that are in the negative lexicon, but for which the $\\log \\phi_{pos,w} > \\log \\phi_{neg,w} + 0.1$. \n",
      "    - (These words are more likely in the positive class, despite being in the negative lexicon.)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "false_pos = set()\n",
      "false_neg = set()\n",
      "weights_instance = weights_nb_alphas[100]\n",
      "for word in poswords:\n",
      "    if ('NEG',word) in weights_instance and ('POS',word) in weights_instance:\n",
      "        if weights_instance['NEG',word] > (weights_instance['POS',word]+0.1):\n",
      "            false_neg.add(word)\n",
      "for word in negwords:\n",
      "    if ('NEG',word) in weights_instance and ('POS',word) in weights_instance:\n",
      "        if weights_instance['POS',word] > (weights_instance['NEG',word]+0.1):\n",
      "            false_pos.add(word)\n",
      "\n",
      "print \"False positives= \",false_pos\n",
      "\n",
      "for word in false_pos:\n",
      "    print word, \"appeared positively \",counts['POS'][word],\"times and negatively,\",counts['NEG'][word],\"times\"\n",
      "    \n",
      "print \"\\n\\nFalse negatives = \",false_neg\n",
      "for word in false_neg:\n",
      "    print word, \"appeared positively \",counts['POS'][word],\"times and negatively,\",counts['NEG'][word],\"times\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False positives=  set(['long', 'drama', 'haunting', 'tragic', 'struggle', 'fun', 'emotional', 'grim'])\n",
        "long appeared positively  115 times and negatively, 90 times\n",
        "drama appeared positively  58 times and negatively, 33 times\n",
        "haunting appeared positively  16 times and negatively, 2 times\n",
        "tragic appeared positively  20 times and negatively, 8 times\n",
        "struggle appeared positively  27 times and negatively, 10 times\n",
        "fun appeared positively  101 times and negatively, 52 times\n",
        "emotional appeared positively  33 times and negatively, 8 times\n",
        "grim appeared positively  14 times and negatively, 0 times\n",
        "\n",
        "\n",
        "False negatives =  set(['funny', 'kind', 'okay', 'pretty', 'just', 'joke', 'wow', 'please', 'like', 'better', 'honestly', 'want', 'talent', 'redeeming', 'wonder'])\n",
        "funny appeared positively  118 times and negatively, 157 times\n",
        "kind appeared positively  81 times and negatively, 102 times\n",
        "okay appeared positively  10 times and negatively, 28 times\n",
        "pretty appeared positively  91 times and negatively, 134 times\n",
        "just appeared positively  491 times and negatively, 659 times\n",
        "joke appeared positively  9 times and negatively, 33 times\n",
        "wow appeared positively  10 times and negatively, 26 times\n",
        "please appeared positively  26 times and negatively, 50 times\n",
        "like appeared positively  579 times and negatively, 676 times\n",
        "better appeared positively  152 times and negatively, 214 times\n",
        "honestly appeared positively  5 times and negatively, 22 times\n",
        "want appeared positively  123 times and negatively, 146 times\n",
        "talent appeared positively  17 times and negatively, 41 times\n",
        "redeeming appeared positively  1 times and negatively, 26 times\n",
        "wonder appeared positively  20 times and negatively, 57 times\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** (7650 only)\n",
      "\n",
      "What do you think is going on here? Pick one of these words, and look for example reviews that contain it (using [grep](http://en.wikipedia.org/wiki/Grep)). \n",
      "\n",
      "Is the word used in the opposite sense, or is there some other explanation?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(explain here)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "The word better was picked from the list of false negatives.\n",
      "Although the word better is in the postive lexicon, often the word better is being used to compare with other movies\n",
      "(in this particular dataset). As a result, other movies are being referred to as \"better\". Therefore, despite being in the\n",
      "positive lexicon, better is classified as negative because it has appeared more frequently in negative reviews.\n",
      "\n",
      "Sample documents include 12255_3.txt and 6962_7.txt.\n",
      "6962_7 refers to a completely different context for better. Here, it is talking about a subject in the movie and \n",
      "therefore reflects no classification of the movie at all.\n",
      "1225_3 referes to the word better in the context of another movie being better. Since this review is negative, the negative\n",
      "weight for the word better goes up.\n",
      "\n",
      "The main reason for these anomalies are due to the absence of semantics in the classification. We are not looking at the \n",
      "context and meaning of the words occuring in the reviews, but going simply by the frequencies. Therefore, all these words\n",
      "are examples to show that a frequency based model has its limitations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}