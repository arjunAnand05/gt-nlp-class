{
 "metadata": {
  "name": "",
  "signature": "sha256:e0816f75646a218cfa82193c4c1defcb716cb07aa88624c8109514ff097903e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 2: Sequence labeling\n",
      "=====================\n",
      "\n",
      "This project focuses on sequence labeling, in the target domain of Twitter part-of-speech tagging.\n",
      "Part (b) focuses on *discriminative* approaches, mainly averaged perceptron and structured perceptron.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer, operator\n",
      "from collections import defaultdict, Counter\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Data processing code\n",
      "\"\"\"\n",
      "def conllSeqGenerator(input_file,max_insts=1000000):\n",
      "    \"\"\" \n",
      "    return an instance generator for a filename.\n",
      "    \n",
      "    The generator yields lists of words and tags.  \n",
      "    No need to change this\n",
      "    \"\"\"\n",
      "    cur_words = []\n",
      "    cur_tags = []\n",
      "    num_insts = 0\n",
      "    with open(input_file) as instances:\n",
      "        for line in instances:\n",
      "            if len(line.rstrip()) == 0:\n",
      "                if len(cur_words) > 0:\n",
      "                    num_insts += 1\n",
      "                    yield cur_words,cur_tags\n",
      "                    cur_words = []\n",
      "                    cur_tags = []\n",
      "            else:\n",
      "                parts = line.rstrip().split()\n",
      "                cur_words.append(parts[0])\n",
      "                if len(parts)>1:\n",
      "                    cur_tags.append(parts[1])\n",
      "                else: cur_tags.append(unknown)\n",
      "        if len(cur_words)>0: \n",
      "            num_insts += 1\n",
      "            yield cur_words,cur_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Define the file names\n",
      "trainfile = 'oct27.train'\n",
      "devfile = 'oct27.dev'\n",
      "testfile = 'oct27.test' # You do not have this for now\n",
      "unknown = \"**UNKNOWN**\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for convenience\n",
      "tr_all = []\n",
      "for i,(words,tags) in enumerate(conllSeqGenerator(trainfile)):\n",
      "    tr_all.append((words,tags))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alltags = set()\n",
      "for i,(words, tags) in enumerate(tr_all):    \n",
      "    for tag in tags:\n",
      "        alltags.add(tag)\n",
      "print alltags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['!', '#', '$', '&', ',', 'A', '@', 'E', 'D', 'G', 'M', 'L', 'O', 'N', 'P', 'S', 'R', 'U', 'T', 'V', 'Y', 'X', 'Z', '^', '~'])\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_tag = '--START--'\n",
      "end_tag = '--END--'\n",
      "trans ='--T--'\n",
      "emit = '--E--'\n",
      "offset = '--OFF--'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Classification-based tagging #\n",
      "\n",
      "First, you will perform tagging as classification problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that in structured prediction, we have the feature function decompose:\n",
      "\n",
      "\\begin{align}\n",
      "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
      "\\vec{f}(\\vec{w},\\vec{y}) & = \\sum_m \\vec{f}(\\vec{w},y_m, y_{m-1}, m)\n",
      "\\end{align}\n",
      "\n",
      "You will explicitly define your feature functions in this way -- even for the classification-based tagger, which won't consider $y_{m-1}$. The features themselves are defined as tuples, as in pset 2a.\n",
      "\n",
      "Here is a simple example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wordFeatures(words,tag,prev_tag,m):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :type words: list\n",
      "    :param tag: a tag\n",
      "    :type tag: string\n",
      "    :type prev_tag: string\n",
      "    :type m: int\n",
      "    '''\n",
      "    out = {(offset,tag):1}\n",
      "    if m < len(words): #we can have m = M, for the transition to the end state\n",
      "        out[(emit,tag,words[m])]=1\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = 'they can can fish'.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordFeatures(sent,'V','V',0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "{('--E--', 'V', 'they'): 1, ('--OFF--', 'V'): 1}"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a** (1 point) Create a new feature function which also includes the final character of the current word, and the final character of the preceding word (if $m > 1$). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curr_suffix = '--curr-suff--'\n",
      "prev_suffix = '--prev-suff--'\n",
      "def wordCharFeatures(words,tag,prev_tag,m):\n",
      "    output = wordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    if m < len(words):\n",
      "        output[(curr_suffix,tag,words[m][-1:])] = 1\n",
      "        if m > 0:\n",
      "            output[(prev_suffix,tag,words[m-1][-1:])] = 1\n",
      "    \n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check desired output\n",
      "print wordCharFeatures(sent,'V','V',1)\n",
      "# no prev-suff feature in this one, because m=0\n",
      "print wordCharFeatures(sent,'V','V',0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{('--curr-suff--', 'V', 'n'): 1, ('--E--', 'V', 'can'): 1, ('--OFF--', 'V'): 1, ('--prev-suff--', 'V', 'y'): 1}\n",
        "{('--curr-suff--', 'V', 'y'): 1, ('--E--', 'V', 'they'): 1, ('--OFF--', 'V'): 1}\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now you will define a classification-based tagger. To get you started, here are some test weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_weights = defaultdict(float)\n",
      "test_tags = ['N','V','V','N']\n",
      "for i in range(len(sent)):\n",
      "    for feat in wordFeatures(sent,test_tags[i],'X',i):\n",
      "        test_weights[feat] = 1\n",
      "    for feat in wordFeatures(sent,'X','X',i):\n",
      "        test_weights[feat] = 1\n",
      "print test_weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'float'>, {('--E--', 'X', 'they'): 1, ('--E--', 'X', 'can'): 1, ('--E--', 'V', 'can'): 1, ('--OFF--', 'N'): 1, ('--OFF--', 'X'): 1, ('--E--', 'X', 'fish'): 1, ('--E--', 'N', 'they'): 1, ('--OFF--', 'V'): 1, ('--E--', 'N', 'fish'): 1})\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use this to find the highest-scoring label\n",
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** (1 point): Define a function that takes a list of words, feature function, dict of weights, and a tagset, and outputs a list of predicted tags (one per word)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classifierTagger(words,featfunc,weights,all_tags):\n",
      "    \"\"\"\n",
      "    :param words: list of words\n",
      "    :param features: function from lists of words and tags to list of features\n",
      "    :param weights: defaultdict of weights\n",
      "    :param all_tags: list of permissible tags\n",
      "    :returns list of tags\n",
      "    \"\"\"\n",
      "    out = [None] * len(words)\n",
      "    for i in range(0,len(words)):\n",
      "        scores = defaultdict(float)\n",
      "        for tag in all_tags:\n",
      "            features = featfunc(words,tag,out[i-1],i)\n",
      "            for feature,value in features.iteritems():    \n",
      "                scores[tag] +=  value * weights[feature]\n",
      "                    \n",
      "        out[i] = argmax(scores)\n",
      "    \n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check from my implementation\n",
      "classifierTagger(sent,wordFeatures,test_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "['N', 'V', 'V', 'N']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's a function that evaluates a tagger on the devset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalTagger(tagger,outfilename,evalfile=devfile):\n",
      "    \"\"\"\n",
      "    :param tagger: a function that takes words and a list of candidate tags, and outputs a list of tags\n",
      "    :param outfilename: a filename to store the output\n",
      "    :param evalfile: a filename of the dev set data\n",
      "    \"\"\"\n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for words,_ in conllSeqGenerator(evalfile):\n",
      "            pred_tags = tagger(words,alltags)\n",
      "            for tag in pred_tags:\n",
      "                print >>outfile, tag\n",
      "            print >>outfile, \"\"\n",
      "    return scorer.getConfusion(evalfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use evalTagger, you'll need pass in a tagger which takes only two arguments. \n",
      "\n",
      "To do this, you can use a lambda expression to pass some arguments to classifierTagger, as shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words,alltags : classifierTagger(words,wordFeatures,test_weights,alltags),'test')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.139539705577\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** (3 points): Apply your averaged perceptron from pset 1b to do part-of-speech tagging. Start by adapting your oneItAvgPerceptron function. You'll have to make some changes:\n",
      "\n",
      "- Replace your call to the predict() function with a call to classifierTagger()\n",
      "- The instanceGenerator now produces word lists and tag lists as instances, instead of feature counts.\n",
      "- You can treat entire sentences as instances, if you want -- this may be slightly easier. This means that you only update the weights after seeing an entire sentence, sort of like a minibatch.\n",
      "- You'll want to add the feature function as an extra argument to both oneItAvgPerceptron and trainAvgPerceptron\n",
      "- return the training accuracy rather than the number of errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def oneItAvgPerceptron(inst_generator,featfunc,weights,wsum,tagset,Tinit=0):\n",
      "    \"\"\"\n",
      "    :param inst_generator: iterator over instances\n",
      "    :param featfunc: feature function on (words, tag_m, tag_m_1, m)\n",
      "    :param weights: default dict\n",
      "    :param wsum: weight sum, for averaging\n",
      "    :param tagset: set of permissible tags\n",
      "    :param Tinit: initial value of t, the counter over instances\n",
      "    :returns weights: a defaultdict of weights\n",
      "    :returns wsum: a defaultdict of weight sums, for averaging (as in pset 1b)\n",
      "    :returns tr_acc: training set accuracy\n",
      "    :return i: a counter of the number of instances seen\n",
      "    \"\"\"\n",
      "    tr_err = 0.0\n",
      "    for i,(words,y_true) in enumerate(inst_generator):\n",
      "        pred_tags = classifierTagger(words,featfunc,weights,tagset)\n",
      "        for k in range(0,len(words)):\n",
      "            if y_true[k] == pred_tags[k]:\n",
      "                continue\n",
      "            tr_err += 1\n",
      "            for tag in [y_true[k],pred_tags[k]]:\n",
      "                feature_set = featfunc(words,tag,tag,k)\n",
      "                for feature,value in feature_set.iteritems():\n",
      "                    if tag == y_true[k]:\n",
      "                        weights[feature] += value\n",
      "                        wsum[feature] += Tinit*value\n",
      "                    \n",
      "                    else:\n",
      "                        weights[feature] -= value\n",
      "                        wsum[feature] -= Tinit*value\n",
      "        Tinit += 1\n",
      "        \n",
      "    # note that i'm computing tr_acc for you, as long as you properly update tr_err\n",
      "    return weights, wsum, 1.-tr_err / float(sum([len(s) for s,t in inst_generator])), i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# here's how it's used. this takes approx 2 seconds for me (using %%timeit)\n",
      "weights,wsum,tr_acc,i = oneItAvgPerceptron(tr_all,wordFeatures,defaultdict(float),defaultdict(float),alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sanity check. The weight sum numbers might be different if you don't treat sentences as instances, which is what I do.\n",
      "print weights[emit,'D','the'], wsum[emit,'D','the']\n",
      "print weights[emit,'N','the'], wsum[emit,'N','the']\n",
      "print weights[emit,'V','like'], wsum[emit,'V','like']\n",
      "print weights[emit,'P','like'], wsum[emit,'P','like']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16.0 2611.0\n",
        "-1.0 -212.0\n",
        "2.0 587.0\n",
        "5.0 942.0\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** (2 points): Now adapt trainAvgPerceptron function to do tagging. This should require fewer changes than oneItAvgPerceptron, but you will have to:\n",
      "\n",
      "- take a feature function as an argument\n",
      "- call evalTagger instead of evalClassifier to get the confusion matrix\n",
      "- don't forget you've modified oneItAvgPerceptron to return the training set accuracy, not the number of errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainAvgPerceptron(N_its,inst_generator,featfunc,tagset):\n",
      "    \"\"\"\n",
      "    :param N_its: number of iterations\n",
      "    :param inst_generator: generate words,tags pairs\n",
      "    :param featfunc: feature function\n",
      "    :param tagset: set of all possible tags\n",
      "    :returns average weights, training accuracy, dev accuracy\n",
      "    \"\"\"\n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    T = 0\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # your code here\n",
      "        T+=1\n",
      "        weights,wsum,tr_acc_i,tr_tot = oneItAvgPerceptron(inst_generator,featfunc,weights,wsum,tagset)\n",
      "        avg_weights = defaultdict(float)\n",
      "        for feature in weights:\n",
      "            avg_weights[feature] = weights[feature] - float(float(wsum[feature])/(T*tr_tot))\n",
      "        \n",
      "        confusion = evalTagger(lambda words, alltags: classifierTagger(words,featfunc,avg_weights,alltags),'perc')        \n",
      "        dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = tr_acc_i\n",
      "        print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "    return avg_weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usage example below. Note that we do about as well as the HMM with just wordCharFeatures, and we're not even considering sequence information yet. This code takes less than 30 seconds for me to run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w, tr_acc, dv_acc = trainAvgPerceptron(10,tr_all,wordCharFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.673439767779 train: 0.523428415076\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.640058055152 train: 0.685477802859\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.689197594858 train: 0.764279362473\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.707443499896 train: 0.824953827211\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.718432510885 train: 0.853615158356\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.708272859216 train: 0.879813940762\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.700186605847 train: 0.89034817703\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.713249015136 train: 0.899445926534\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.716981132075 train: 0.903892195089\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.718432510885 train: 0.904713044668\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4e** (3 points): Make it better! Design a killer feature set that improves performance on the devset.\n",
      "\n",
      "I'm able to get above 84% on the dev set, without going too crazy. Warning: my additional features slow things down considerably."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "suffix = '--suff--'\n",
      "prefix = '--pref--'\n",
      "word_trans = '--prevword--'\n",
      "\n",
      "def yourFeatures(words,tag,prev_tag,m):\n",
      "    output = wordCharFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    if m<len(words) and len(words[m]) > 2:\n",
      "        output[(suffix,tag,words[m][-2:])] = 1\n",
      "        output[(prefix,tag,words[m][:2])] = 1\n",
      "        output[(tag,words[m][-2:])] = 1\n",
      "        output[(tag,words[m][:2])] = 1\n",
      "        \n",
      "    if m==0:\n",
      "        output[(start_tag,tag,words[m])] = 1\n",
      "        output[(trans,start_tag,tag)] = 1\n",
      "        \n",
      "    if m==len(words)-1:\n",
      "        output[(tag,end_tag,words[m])] = 1\n",
      "        output[(trans,tag,end_tag)] = 1\n",
      "    \n",
      "    if m>0 and m<(len(words)-1):\n",
      "        output[(trans,prev_tag,tag)] = 1\n",
      "        output[(word_trans,tag,words[m-1])] = 1\n",
      "    \n",
      "    output[(first_char,tag,words[m][0])] = 1\n",
      "        \n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w, tr_acc, dv_acc = trainAvgPerceptron(10,tr_all,yourFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.77814638192 train: 0.658526575005\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.78125647937 train: 0.79430877625\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.771718847191 train: 0.840960394008\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.785195936139 train: 0.881387235789\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.786647314949 train: 0.901498050482\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.790794111549 train: 0.919488337096\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.787269334439 train: 0.929406936179\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.785610615799 train: 0.940351597236\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.787269334439 train: 0.938641493946\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.794940908148 train: 0.947328818661\n"
       ]
      }
     ],
     "prompt_number": 185
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5. Discriminative Structure Prediction #\n",
      "\n",
      "Now you will implement a Structured Perceptron, which is trained to find the optimal *sequence* $\\vec{y} = \\text{arg}\\max_\\vec{y} \\theta^{\\top} \\vec{f}(\\vec{w},\\vec{y})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A key difference from the classification-based setting is that we compute features over the entire sequence.\n",
      "\n",
      "**Deliverable 5a** (0.5 points): Implement a function seqFeatures, which takes a list of words, a list of tags, and a feature function, and returns a dictionary of features and their counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def seqFeatures(words,tags,featfunc):\n",
      "    '''\n",
      "    :param words: a list of words\n",
      "    :param tags: a list of tags\n",
      "    :param featfunc: a function to compute f(words,tag_m,tag_{m-1},m)\n",
      "    :returns list of features\n",
      "    '''\n",
      "    allfeats = defaultdict(float)\n",
      "    for i in range(0,len(words)):\n",
      "        if i == 0:\n",
      "            feature_set = featfunc(words,tags[i],start_tag,i)\n",
      "        else:\n",
      "            feature_set = featfunc(words,tags[i],tags[i-1],i)\n",
      "            \n",
      "        for feature in feature_set:\n",
      "            allfeats[feature] += 1\n",
      "    allfeats[(offset,end_tag)] = 1\n",
      "    # your code here\n",
      "    return allfeats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqFeatures(sent,['N','V','V','N'],wordFeatures)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "defaultdict(<type 'float'>, {('--E--', 'V', 'can'): 2.0, ('--OFF--', 'N'): 2.0, ('--OFF--', '--END--'): 1, ('--E--', 'N', 'they'): 1.0, ('--OFF--', 'V'): 2.0, ('--E--', 'N', 'fish'): 1.0})"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5b** (0.5 points): now create a new feature function wordTransFeatures, which adds tag-to-tag transition features to wordFeatures. Note that this feature set is identical to what the HMM uses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wordTransFeatures(words,tag,prev_tag,m):\n",
      "    output = wordFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    if m<len(words):\n",
      "        output[(trans,tag,prev_tag)] = 1\n",
      "    if m == (len(words) -1):\n",
      "        output[(trans,end_tag,tag)] = 1\n",
      "\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "seqFeatures(sent,['N','V','V','N'],wordTransFeatures)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "defaultdict(<type 'float'>, {('--T--', 'V', 'V'): 1.0, ('--E--', 'V', 'can'): 2.0, ('--T--', 'N', 'V'): 1.0, ('--OFF--', 'N'): 2.0, ('--OFF--', '--END--'): 1, ('--T--', 'N', '--START--'): 1.0, ('--E--', 'N', 'they'): 1.0, ('--OFF--', 'V'): 2.0, ('--E--', 'N', 'fish'): 1.0, ('--T--', '--END--', 'N'): 1.0, ('--T--', 'V', 'N'): 1.0})"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5c** (1 point): copy in your viterbiTagger from part 2a. If you implemented it correctly, you should be able to use it without modification here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def viterbiTagger(words,feat_func,weights,all_tags,debug=False):\n",
      "    trellis = [None] * len(words) #hint: store the $v$ table here \n",
      "    pointers = [None] * len(words) #hint: store the $b$ table here\n",
      "    output = [None] * len(words) #hint: store the output here. build this last.\n",
      "    previous_tag = start_tag\n",
      "    for i in range(0,len(words)):\n",
      "        for tag in all_tags:\n",
      "            v = 0\n",
      "            reqd_features = feat_func(words,tag,previous_tag,i)\n",
      "            for feature,value in reqd_features.iteritems():\n",
      "                v += weights[feature]\n",
      "            if v > trellis[i] or trellis[i] is None:\n",
      "                trellis[i] = v\n",
      "                pointers[i] = tag\n",
      "        previous_tag = pointers[i]\n",
      "    \n",
      "    best_score = sum(trellis)\n",
      "    for i in range(0,len(words)):\n",
      "        output[i] = words[i],\"/\",pointers[i]\n",
      "        \n",
      "    return pointers,best_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sanity check\n",
      "viterbiTagger(['they','can','can','fish'],wordTransFeatures,test_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "(['N', 'V', 'V', 'N'], 8.0)"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5d** (5 points): create a function oneItAvgStructPerceptron, which performs a single iteration of averaged structured perceptron. It should be similar to your oneItAvgPerceptron, but will have to be different in some ways to reflect the structured prediction scenario.\n",
      "\n",
      "- To make predictions, you must call your viterbiTagger function\n",
      "- To compute the features for a given sequence of words and tags, you must call you seqFeatures function\n",
      "- As above, output the training accuracy, not the number of training errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def oneItAvgStructPerceptron(inst_generator,\n",
      "                             featfunc,\n",
      "                             weights,\n",
      "                             wsum,\n",
      "                             tagset,\n",
      "                             Tinit=0):\n",
      "    \"\"\"\n",
      "    :param inst_generator: A generator of (words,tags) tuples\n",
      "    :param tagger: A function from (words, weights) to tags\n",
      "    :param features: A function from (words, tags) to a dict of features and weights\n",
      "    :param weights: A defaultdict of weights\n",
      "    :param wsum: A defaultdict of weight sums\n",
      "    :param Tinit: the initial value of the $t$ counter at the beginning of this iteration\n",
      "    :returns weights: a defaultdict of weights\n",
      "    :returns wsum: a defaultdict of weight sums, for averaging\n",
      "    :returns tr_acc: the training accuracy\n",
      "    :returns i: the number of instances (sentences) seen\n",
      "    \"\"\"\n",
      "    tr_err = 0.\n",
      "    tr_tot = 0.\n",
      "      # your code\n",
      "    for i,(words,y_true) in enumerate(inst_generator):\n",
      "        tr_tot += len(words)\n",
      "        \n",
      "        pred_tags = viterbiTagger(words,featfunc,weights,tagset)[0]\n",
      "        #Compute no.of erroneous tag predictions\n",
      "        correct_preds = 0\n",
      "        for k in range(0,len(words)):\n",
      "            if pred_tags[k] == y_true[k]:\n",
      "                correct_preds += 1\n",
      "        \n",
      "        Tinit +=1\n",
      "        #Continue if all correct\n",
      "        if correct_preds == len(words):\n",
      "            continue \n",
      "        \n",
      "        tr_err += len(words) - correct_preds\n",
      "        for feature,value in seqFeatures(words,pred_tags,featfunc).iteritems():\n",
      "            weights[feature] -= value\n",
      "            wsum[feature] -= Tinit*value\n",
      "            \n",
      "        for feature,value in seqFeatures(words,y_true,featfunc).iteritems():\n",
      "            weights[feature] += value\n",
      "            wsum[feature] += Tinit*value                \n",
      "    \n",
      "    return weights, wsum, (1.0-(tr_err/tr_tot)), i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sanity check. Note that I'm only considering the first 100 sentences, and it still takes 6 seconds. You may want to debug on an even smaller subset of the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights,wsum,tr_acc,i = oneItAvgStructPerceptron(tr_all[:100],wordTransFeatures,defaultdict(float),defaultdict(float),alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "weights,wsum,tr_acc,i = oneItAvgStructPerceptron(tr_all[:100],wordTransFeatures,defaultdict(float),defaultdict(float),alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 443 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tag1 in list(alltags)[:7]:\n",
      "    for tag2 in list(alltags)[:7]:\n",
      "        if weights[trans,tag1,tag2] != 0:\n",
      "            print tag1,tag2,weights[(trans,tag1,tag2)],wsum[trans,tag1,tag2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "! ! -29.0 -11.0\n",
        "! , 4.0 131.0\n",
        "! @ 6.0 21.0\n",
        "# # -5.0 -335.0\n",
        "# , -5.0 -484.0\n",
        "$ $ -14.0 -208.0\n",
        "$ , 3.0 4.0\n",
        "& A 1.0 74.0\n",
        "& @ 2.0 129.0\n",
        ", ! 7.0 156.0\n",
        ", # -5.0 -366.0\n",
        ", $ 5.0 222.0\n",
        ", , -7.0 47.0\n",
        ", A 6.0 433.0\n",
        ", @ -1.0 -88.0\n",
        "A ! 2.0 110.0\n",
        "A , -1.0 -135.0\n",
        "A A -9.0 -79.0\n",
        "@ ! 1.0 96.0\n",
        "@ & 1.0 46.0\n",
        "@ , 1.0 47.0\n",
        "@ @ -15.0 -146.0\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sample output"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5e** (2 points): Implement trainAvgStructPerceptron. This will be quite similar to your trainAvgPerceptron from ps1b, but will have to take slightly different arguments to handle the structured prediction case. Don't forget to use evalTagger to produce output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainAvgStructPerceptron(N_its,inst_generator,featfunc,tagset):\n",
      "    \"\"\"\n",
      "    :param N_its: number of iterations\n",
      "    :param inst_generator: A generator of (words,tags) tuples\n",
      "    :param tagger: A function from (words, weights) to tags\n",
      "    :param features: A function from (words, tags) to a dict of features and weights\n",
      "    :returns weights: defaultdict of weights\n",
      "    :returns tr_acc: training accuracy\n",
      "    :returns dv_acc: dev set accuracy\n",
      "    \"\"\"\n",
      "\n",
      "    tr_acc = [None]*N_its\n",
      "    dv_acc = [None]*N_its\n",
      "    T = 0\n",
      "    weights = defaultdict(float)\n",
      "    wsum = defaultdict(float)\n",
      "    for i in xrange(N_its):\n",
      "        # your code here\n",
      "        T+=1\n",
      "        # note that I call evalTagger to produce the dev set results\n",
      "        weights,wsum,tr_acc_i,tr_tot = oneItAvgStructPerceptron(inst_generator,featfunc,weights,wsum,tagset)\n",
      "        avg_weights = defaultdict(float)\n",
      "        for feature in weights:\n",
      "            avg_weights[feature] = weights[feature] - float(float(wsum[feature])/(T*tr_tot))\n",
      "        \n",
      "        \n",
      "        confusion = evalTagger(lambda words,tags : viterbiTagger(words,featfunc,weights,tags)[0],'sp.txt')\n",
      "        dv_acc[i] = scorer.accuracy(confusion)\n",
      "        tr_acc[i] = tr_acc_i#1. - tr_err/float(sum([len(s) for s,t in inst_generator]))\n",
      "        print i,'dev:',dv_acc[i],'train:',tr_acc[i]\n",
      "    return weights, tr_acc, dv_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code should roughly reproduce this sanity check. It may be a little slow, so we'll just test on the first 50 instances.\n",
      "# While you're debugging your code, you can run on even smaller datasets.\n",
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(5,tr_all[:50],wordTransFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.251917893427 train: 0.165354330709\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.362844702467 train: 0.363779527559\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.318888658511 train: 0.497637795276\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.379639228696 train: 0.596850393701\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.413850300643 train: 0.766929133858\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run your code on all the training data, using wordTransFeatures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(20,tr_all,wordTransFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.568318473979 train: 0.01\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.586771718847 train: 0.042\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.562305618909 train: 0.081\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.591125855277 train: 0.143\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.656230561891 train: 0.21\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.652291105121 train: 0.251\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.639643375492 train: 0.304\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.647314949202 train: 0.316\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.625751606884 train: 0.347\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.685880157578 train: 0.342\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.66970765084 train: 0.38\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.66659755339 train: 0.39\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.681526021149 train: 0.375\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.66514617458 train: 0.391\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.658718639851 train: 0.405\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.643997511922 train: 0.408\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.648351648352 train: 0.427\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.6680489322 train: 0.417\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.654364503421 train: 0.425\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.641094754302 train: 0.423\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 5f** (3 points): Implement a better feature set for structured prediction. For speed reasons, you might not want to use all the features you used in 4e, but try to get as good an accuracy as you can. Last year I was able to get my structured perceptron to work a little better than my best classifier, but this year my classifier is (slightly) better!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "suffix = '--suff--'\n",
      "prefix = '--pref--'\n",
      "word_trans = '--prevword--'\n",
      "\n",
      "def yourHMMFeatures(words,tag,prev_tag,m):\n",
      "    output = wordTransFeatures(words,tag,prev_tag,m) #start with the features from wordFeatures\n",
      "    if m<len(words) and len(words[m]) > 2:\n",
      "        output[(suffix,tag,words[m][-2:])] = 1\n",
      "        output[(prefix,tag,words[m][:2])] = 1\n",
      "        output[(tag,words[m][-2:])] = 1\n",
      "        output[(tag,words[m][:2])] = 1\n",
      "        \n",
      "    if m==0:\n",
      "        output[(start_tag,tag,words[m])] = 1\n",
      "        output[(trans,start_tag,tag)] = 1\n",
      "        \n",
      "    if m==len(words)-1:\n",
      "        output[(tag,end_tag,words[m])] = 1\n",
      "        output[(trans,tag,end_tag)] = 1\n",
      "    \n",
      "    if m>0 and m<(len(words)-1):\n",
      "        output[(trans,prev_tag,tag)] = 1\n",
      "        output[(word_trans,tag,words[m-1])] = 1\n",
      "    \n",
      "    output[(first_char,tag,words[m][0])] = 1\n",
      "        \n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta,tr_acc,dv_acc = trainAvgStructPerceptron(15,tr_all,yourHMMFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.727348123575 train: 0.633627471099\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.735019697284 train: 0.773103495451\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.748704126063 train: 0.824132977632\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.762803234501 train: 0.853341541829\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.748911465893 train: 0.880087557288\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.752228903172 train: 0.900061563718\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.769023429401 train: 0.908680484301\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.770474808211 train: 0.917367809016\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.759485797222 train: 0.923318968466\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.743520630313 train: 0.93583692455\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.763425253991 train: 0.940009576578\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.754924320962 train: 0.943840207949\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.768608749741 train: 0.943566591422\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.770682148041 train: 0.951911895478\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.746423387933 train: 0.949654559135\n"
       ]
      }
     ],
     "prompt_number": 199
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 6. Error analysis #\n",
      "\n",
      "**Deliverable 6** (5 points; CS 7650 only). The scorer.py script produces a confusion matrix, which shows the most common types of errors. Consider your best tagger in any part of the assignment, and identify the three most frequent errors (e.g., N classified as V). Find an example sentence in your tagger has made each type of error, and explain why you think it made the mistake, and how it could be fixed. (If you are feeling competitive, you can then use this information to go back and try to improve your features.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w, tr_acc, dv_acc = trainAvgPerceptron(10,tr_all,yourFeatures,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 dev: 0.760937176032 train: 0.643204049525\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.767572050591 train: 0.758396607155\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.761351855691 train: 0.796771324988\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.77545096413 train: 0.823038511526\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.77607298362 train: 0.852657500513\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.78167115903 train: 0.856214515357\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.78250051835 train: 0.871605444969\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.77793904209 train: 0.88480744237\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.787061994609 train: 0.887680415897\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dev: 0.791830810699 train: 0.899924755455\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words, alltags: classifierTagger(words,yourFeatures,w,alltags),'test')        \n",
      "mistakes = defaultdict(int)\n",
      "for key,value in confusion.iteritems():\n",
      "    if key[0] != key[1]:\n",
      "        mistakes[key] = value\n",
      "\n",
      "print \"Three most common mistakes are - \"  #(actual tag, classified tag)\n",
      "for i in range(0,3):\n",
      "    top_mistake = argmax(mistakes)\n",
      "    print top_mistake,\" = \",mistakes[top_mistake]\n",
      "    del mistakes[top_mistake]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Three most common mistakes are - \n",
        "('N', 'V')  =  115\n",
        "('^', 'V')  =  62\n",
        "('^', 'N')  =  58\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1)Misclassified N as V - @ciaranyreE it was on football wives(wrongly classified as verb) one of the players and his wife own smash burger\n",
      "This is most likely happening because of the \"es\" suffix feature which is fairly common in verbs. This can be fixed by considering a  plurals feature as well.\n",
      "\n",
      "2)Misclassified ^ as V - Senate #ArtsGrades are in! See who passed and who made the Dirty Dozen(wrongly classified as verb).#arts URL via @ArtsActionFund - This is happening because of the previous tag being classified as a noun. Therefore, probability favours the tag after\n",
      "a noun being a verb, rather than being a noun. This is basically a failure to recognise entities.\n",
      "\n",
      "3)Misclassified ^ as N - Senate #ArtsGrades are in! See who passed and who made the Dirty(wrongly classified as noun) Dozen.#arts URL via @ArtsActionFund - This is happening because the previous tag being D. As a result, likelihood of a noun following a 'D' is more than that of a proper noun. This can be fixed by assigning more weight to the capitalisation feature than the prev tag feature for the noun case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#After correction\n",
      "confusion = evalTagger(lambda words, alltags: classifierTagger(words,yourFeatures,w_new,alltags),'test')        \n",
      "mistakes = defaultdict(int)\n",
      "for key,value in confusion.iteritems():\n",
      "    if key[0] != key[1]:\n",
      "        mistakes[key] = value\n",
      "\n",
      "print \"Three most common mistakes are - \"  #(actual tag, classified tag)\n",
      "for i in range(0,3):\n",
      "    top_mistake = argmax(mistakes)\n",
      "    print top_mistake,\" = \",mistakes[top_mistake]\n",
      "    del mistakes[top_mistake]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Three most common mistakes are - \n",
        "('^', 'N')  =  69\n",
        "('V', 'N')  =  61\n",
        "('~', ',')  =  57\n"
       ]
      }
     ],
     "prompt_number": 188
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7. Bakeoff! #\n",
      "\n",
      "48 hours before the assignment is due, we will send you unlabeled test data. Your job is to produce a response file that I can evaluate. I'll present the results in class and give the best scorers a chance to explain what they did.\n",
      "\n",
      "** Deliverable 7 ** (3 points) Run your best system from any part of the assignment on the test data. You may also try to improve your system in other ways, such as by using Passive-Aggressive to learn the weights; however, in Deliverable 5f you specifically need to improve the system by designing better features.\n",
      "\n",
      "Rename your response file as lastname-firstname.response, and include it in your submission on T-Square. (Please get this filename right, otherwise we may miss your submission to the bakeoff.) The top scores will be announced inclass.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfilename='anand-arjun.response'\n",
      "with open(outfilename,'w') as outfile:\n",
      "    for words,nothing in conllSeqGenerator(testfile,max_insts=1000000):\n",
      "        pred_tags = classifierTagger(words,yourFeatures,w_new,alltags)\n",
      "        for tag in pred_tags:\n",
      "            print >>outfile, tag\n",
      "        print >>outfile, \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    }
   ],
   "metadata": {}
  }
 ]
}