{
 "metadata": {
  "name": "",
  "signature": "sha256:f00bd4c5bd3db0ccabafbe06754a34b86f05804bf91b51835bde17bf469ac01a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 2: Sequence labeling\n",
      "=====================\n",
      "\n",
      "This project focuses on sequence labeling. \n",
      "Part (a) focuses on *generative* approaches, including Naive Bayes and the Hidden Markov Model (HMM).\n",
      "The target domain is Twitter part-of-speech tagging."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer, operator, itertools\n",
      "from collections import defaultdict, Counter\n",
      "from itertools import chain\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "\n",
      "Download the train data and dev data from [Github](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/projects/proj-2). \n",
      "The test data will be released around 48 hours before the deadline for Pset 2b.\n",
      "The part-of-speech tags are defined in the [ACL2011 paper](http://www.ark.cs.cmu.edu/TweetNLP/gimpel+etal.acl11.pdf) \n",
      "and the [NAACL 2013 paper](http://www.ark.cs.cmu.edu/TweetNLP/owoputi+etal.naacl13.pdf), \n",
      "which also describe the data and gives some state-of-art results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Data processing code\n",
      "\"\"\"\n",
      "def conllSeqGenerator(input_file,max_insts=1000000):\n",
      "    \"\"\" return an instance generator for a filename\n",
      "    \n",
      "    The generator yields lists of words and tags.  \n",
      "    \"\"\"\n",
      "    cur_words = []\n",
      "    cur_tags = []\n",
      "    num_insts = 0\n",
      "    with open(input_file) as instances:\n",
      "        for line in instances:\n",
      "            if len(line.rstrip()) == 0:\n",
      "                if len(cur_words) > 0:\n",
      "                    num_insts += 1\n",
      "                    yield cur_words,cur_tags\n",
      "                    cur_words = []\n",
      "                    cur_tags = []\n",
      "            else:\n",
      "                parts = line.rstrip().split()\n",
      "                cur_words.append(parts[0])\n",
      "                if len(parts)>1:\n",
      "                    cur_tags.append(parts[1])\n",
      "                else: cur_tags.append(unk)\n",
      "        if len(cur_words)>0: \n",
      "            num_insts += 1\n",
      "            yield cur_words,cur_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Define the file names\n",
      "trainfile = 'oct27.train'\n",
      "devfile = 'oct27.dev'\n",
      "testfile = 'oct27.test' # You do not have this for now\n",
      "offset = \"**OFFSET**\"\n",
      "unknown = \"**UNKNOWN**\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is a demo code for using function \"conllSeqGenerator()\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Demo\n",
      "alltags = set()\n",
      "for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)):    \n",
      "    for tag in tags:\n",
      "        alltags.add(tag)\n",
      "print alltags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['!', '#', '$', '&', ',', 'A', '@', 'E', 'D', 'G', 'M', 'L', 'O', 'N', 'P', 'S', 'R', 'U', 'T', 'V', 'Y', 'X', 'Z', '^', '~'])\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1a** (1 point): Use the Counter class to identify the most common three words for each tag, in the training set. \n",
      "The most_common() function of the Counter class will help you here. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "tag_word_counts = defaultdict(lambda : Counter()) # hint\n",
      "for words,tags in conllSeqGenerator(trainfile):\n",
      "    for i in range(0,len(words)):\n",
      "        tag_word_counts[tags[i]] += Counter({words[i]:1})\n",
      "        \n",
      "for tag,counter in tag_word_counts.iteritems():\n",
      "    print tag, \" - \" , counter.most_common(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "!  -  [('lol', 54), ('Lol', 22), ('oh', 10)]\n",
        "#  -  [('#nowplaying', 3), ('#np', 2), ('#tcot', 2)]\n",
        "$  -  [('one', 32), ('4', 6), ('2010', 6)]\n",
        "&  -  [('and', 117), ('&', 39), ('but', 31)]\n",
        ",  -  [('.', 427), ('!', 244), (',', 225)]\n",
        "A  -  [('good', 24), ('new', 22), ('more', 13)]\n",
        "@  -  [('@Fresh32Prince89', 6), ('@lil_jeezy_85', 2), ('@ResourcefulMom', 2)]\n",
        "E  -  [(':)', 28), ('<3', 10), (';)', 8)]\n",
        "D  -  [('the', 236), ('a', 165), ('my', 89)]\n",
        "G  -  [('smh', 9), ('|', 7), ('-', 7)]\n",
        "M  -  [(\"momma's\", 1), ('#LebronShould', 1), (\"Ricochet's\", 1)]\n",
        "L  -  [(\"I'm\", 42), ('its', 24), ('im', 15)]\n",
        "O  -  [('I', 258), ('you', 135), ('it', 87)]\n",
        "N  -  [('day', 19), ('time', 18), ('people', 17)]\n",
        "P  -  [('to', 231), ('of', 112), ('for', 101)]\n",
        "S  -  [('mans', 1), (\"judge's\", 1), ('year\\xe2\\x80\\x99s', 1)]\n",
        "R  -  [('just', 56), ('not', 27), ('now', 26)]\n",
        "U  -  [('http', 4), (':/', 1), ('http://tinyurl.com/2dt3o7n', 1)]\n",
        "T  -  [('out', 29), ('up', 26), ('on', 8)]\n",
        "V  -  [('is', 105), ('are', 52), ('have', 48)]\n",
        "Y  -  [(\"there's\", 2)]\n",
        "X  -  [('all', 6), ('There', 4), ('there', 2)]\n",
        "Z  -  [(\"Obamacare's\", 1), (\"Party's\", 1), (\"Wayne's\", 1)]\n",
        "^  -  [('Heat', 8), ('Halloween', 5), ('twitter', 5)]\n",
        "~  -  [('RT', 229), (':', 207), ('...', 59)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Most-common-tag baseline ##\n",
      "\n",
      "Build a classifier that chooses the most common tag for each word.\n",
      "\n",
      "- You should implement your classifier in terms of a set of weights, as in pset 1\n",
      "- Prediction should use your predict() function from pset 1\n",
      "- For unseen words, the classifier should choose the tag with the most **unique** word types"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# paste your predict function here\n",
      "# should return two outputs: the highest-scoring label, and the scores for all labels\n",
      "def predict(instance,weights,labels):\n",
      "    scores = defaultdict(float)\n",
      "    #word_weights = set(itertools.chain.from_iterable(weights))\n",
      "    for word,feature in instance.iteritems():\n",
      "        word_missing = True\n",
      "        for label in labels:\n",
      "            if (label,word) in weights:\n",
      "                word_missing = False\n",
      "                scores[label] += weights[(label,word)] * feature\n",
      "        if word_missing:\n",
      "            for label in labels: scores[label] = weights[(label,offset)] * feature        \n",
      "    argmax = lambda scores : max(scores.iteritems(),key=operator.itemgetter(1))[0]\n",
      "    return argmax(scores),scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "evalTagger evaluates a tagger. It takes three arguments:\n",
      "\n",
      "- a tagger, which is a **function** taking a list of words and a tagset as arguments\n",
      "- an output filename\n",
      "- a test file\n",
      "\n",
      "You will want to use lambda expressions to create the first argument for this function, as shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalTagger(tagger,outfilename,testfile=devfile):    \n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for words,_ in conllSeqGenerator(testfile):\n",
      "            pred_tags = tagger(words,alltags)\n",
      "            for tag in pred_tags:\n",
      "                print >>outfile, tag\n",
      "            print >>outfile, \"\"\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's how it works. I provide a tagger that labels everything as a noun."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the list comprehension creates a list of 'N' equal to the length of the list of words\n",
      "noun_tagger = lambda words, alltags : ['N' for word in words]\n",
      "confusion = evalTagger(noun_tagger,'nouns')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.136844287788\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function called classifier tagger, with the following signature:\n",
      "- **input 1** a list of words\n",
      "- **input 2** a dict of weights\n",
      "- **input 3** a list of all possible tags\n",
      "- **output 1** a list of predicted tags\n",
      "\n",
      "Your function should call predict, using the weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function called classifierTagger\n",
      "def classifierTagger(words,weights,all_tags):\n",
      "    tags = []\n",
      "    for word in words:\n",
      "        tags.append(predict({word:1},weights,all_tags)[0])        \n",
      "    return tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1b** (1 point) Use your classifierTagger to reproduce the \"always noun\" tagger above, this time by using the offset weight. \n",
      "\n",
      "**Sanity check** the accuracy should be the same as above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code: set the weights\n",
      "your_weights = defaultdict(float)\n",
      "for tag in alltags:\n",
      "    if tag == 'N':\n",
      "        your_weights[(tag,offset)] = 1\n",
      "    else:\n",
      "        your_weights[(tag,offset)] = 0\n",
      "    \n",
      "#print your_weights\n",
      "noun_tagger_2 = lambda words, alltags: classifierTagger(words, your_weights, alltags)\n",
      "\n",
      "print scorer.accuracy(evalTagger(noun_tagger_2,'nouns'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.136844287788\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1c** (2 points) Implement a tagger that selects the most common tag for each word. \n",
      "- You should use classifierTagger for this, so select the weights appropriately\n",
      "- To do this, you'll want to construct a bunch of counters, similar to what you did above.\n",
      "- For words that do not appear in the training data, select the tag which applies to the most word *types* in the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "weights = defaultdict(float)\n",
      "tag_counts = {}\n",
      "for tag,word_count in tag_word_counts.iteritems():\n",
      "    tag_counts[tag] = sum(word_count.values())\n",
      "    for word,count in word_count.iteritems():\n",
      "        weights[(tag,word)] = count\n",
      "   \n",
      "most_common_tag = argmax(tag_counts)\n",
      "weights[(most_common_tag,offset)] = tag_counts[most_common_tag]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check** my accuracy is approximately 70%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words,alltags : classifierTagger(words,weights,alltags),'mcc')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.673232427949\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2 Naive Bayes Classification #\n",
      "\n",
      "\n",
      "Write a function that builds a Naive Bayes classifier to perform supervised classification.\n",
      "\n",
      "- The base features should just be the word itself, plus an offset feature. \n",
      "- The output of this function should be weights that you can plug into the predict function that you wrote last time\n",
      "- An input should be a smoothing value $\\alpha$\n",
      "- **Hint**: You may want to use the counters that you built in problem 1a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getNBWeights(alpha):\n",
      "    weights = defaultdict(lambda : -1000.) #default value is a low log-probability\n",
      "    # your code here\n",
      "    return weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: your code should give the same results I get below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w1 = getNBWeights(0.1)\n",
      "# print total probability mass for a tag\n",
      "print sum([np.exp(w1[('N',word)]) for word in word_counters.keys()])\n",
      "# print some common values\n",
      "print w1[('N','breakfast')], w1[('V','breakfast')], w1[('A','smart')], w1[('D','the')], w1[('!',offset)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2** (3 points): run the code below to evaluate your naive bayes tagger on the development set.\n",
      "\n",
      "**Sanity check**: you may do a little worse than the most-common tag classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dev_acc = dict()\n",
      "for alpha in [1e-4,1e-3,1e-2,1e-1,1e0,1e1]:\n",
      "    nb_weights = getNBWeights(alpha)\n",
      "    confusion = evalTagger(lambda words, alltags : classifierTagger(words,nb_weights,alltags),'nb')\n",
      "    dev_acc[alpha] = scorer.accuracy(confusion)\n",
      "    print alpha,dev_acc[alpha]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3 Viterbi Algorithm #\n",
      "\n",
      "In this section you will implement the Viterbi algorithm. As a reminder, here's the math:\n",
      "\n",
      "$\\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
      "\\begin{align*}\n",
      "\\vec{w}^{\\top}\\vec{f}(\\vec{x},\\vec{y}) = & \\sum_i \\vec{w}^{\\top} \\vec{f}(\\vec{x},y_i,y_{i-1},i) \\\\\n",
      "v(y,0) = & \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,\\diamond,0)\\\\\n",
      "b(y,0) = & \\diamond \\\\\n",
      "v(y,i) = & \\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
      "b(y,i-1) = & \\text{arg}\\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get warmed up, let's work out an example by hand. There are only two tags, \n",
      "N and V. Here are the parameters:\n",
      "\n",
      "| | Value |\n",
      "| ------------- |:-------------:|\n",
      "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
      "| $\\log P_E(\\cdot|V)$ | they: -10, can: -2, fish: -3 |\n",
      "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
      "| $\\log P_T(\\cdot|V)$ | N: -1, V: -4, END: -3 |\n",
      "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-1 |\n",
      "\n",
      "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the translation probability.\n",
      " \n",
      "In class we discuss the sentence *They can fish*. Now work out a more complicated example: \"*They can can fish*\".\n",
      " \n",
      "** Deliverable 3a ** (1 point) Show the trellis-like table, and give the score for the best best scoring path(s). After you work out the trellis by hand, you should be able to fill the following table.\n",
      "\n",
      "\n",
      "** Sanity check ** There are two paths that each score -18."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Fill your answer in the following table)*\n",
      "\n",
      "|POS tag| START  | they | can | can | fish | END |\n",
      "|-------|:-------|:-----|:----|:----|:-----|:---:|\n",
      "| N     |    0   |      |     |     |      | -18 |\n",
      "| V     |    0   |      |     |     |      | -18 |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing Viterbi ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can see how these are used in the example weights below\n",
      "start_tag = '--START--'\n",
      "end_tag = '--END--'\n",
      "trans ='--TRANS--'\n",
      "emit = '--EMISSION--'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function computes the HMM features for the function $\\vec{f}(\\vec{x},y,y',i)$. \n",
      "- You will call it in your viterbi tagger. \n",
      "- Note that it returns both an emission and transition feature, except for the last word, where it returns only a transition feature. \n",
      "- Also note that transition and emission features are specially marked"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hmm_feats(words,curr_tag,prev_tag,i):\n",
      "    if i < len(words):\n",
      "        return [(curr_tag,words[i],emit),(curr_tag,prev_tag,trans)]\n",
      "    else: return [(curr_tag,prev_tag,trans)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some predefined weights, corresponding to problem 3a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "defined_weights = {('N','they',emit):-1,('N','can',emit):-3,('N','fish',emit):-3,\\\n",
      "                        ('V','they',emit):-10,('V','can',emit):-2,('V','fish',emit):-3,\\\n",
      "                        ('N','N',trans):-5,('V','N',trans):-2,(end_tag,'N',trans):-3,\\\n",
      "                        ('N','V',trans):-1,('V','V',trans):-4,(end_tag,'V',trans):-3,\\\n",
      "                        ('N',start_tag,trans):-1,('V',start_tag,trans):-1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define viterbiTagger\n",
      "\n",
      "- **Input 1**: a list of words\n",
      "- **Input 2**: a feature function, like hmm_feats\n",
      "- **Input 3**: a dict of weights\n",
      "- **Input 4**: a list of all possible tags\n",
      "- **Output 1**: the best-scoring sequence\n",
      "- **Output 2**: the score of the best-scoring sequence\n",
      "\n",
      "Hint: you can represent the trellis and the back pointers as lists of dicts. You will want to do some special handling for the first and last words; otherwise, just iterate "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def viterbiTagger(words,feat_func,weights,all_tags,debug=False):\n",
      "    trellis = [None] * len(words) #hint: store the $v$ table here \n",
      "    pointers = [None] * len(words) #hint: store the $b$ table here\n",
      "    output = [None] * len(words) #hint: store the output here. build this last.\n",
      "    # your code here\n",
      "    return output,best_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** (1 point) run you viterbi tagger on the example in 3a, using the code below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keep this code\n",
      "viterbiTagger(['they','can','can','fish'],hmm_feats,defined_weights,['N','V'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3c** (1 point) run your Viterbi on the following example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = 'they can can can can can can can fish'.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "viterbiTagger(sent,hmm_feats,defined_weights,['N','V'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now estimate the weights of a hidden Markov model. \n",
      "- You have already estimated the emission weights $\\log P(w | y)$, in your solution to problem 2. Use your solution with $\\alpha=0.001$\n",
      "- Estimate the transition probabilities from the training data, using the maximum-likelihood estimates (no smoothing)\n",
      "- Don't forget transitions from the start state and to the end state"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb_weights = getNBWeights(0.001) #compute naive bayes weights\n",
      "# convert nb weights to hmm weights\n",
      "hmm_weights = defaultdict(lambda : -1000.)\n",
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute tag-to-tag transition counts, add them to hmm_weights\n",
      "tag_trans_counts = defaultdict(lambda : Counter())\n",
      "# your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: here are some weights that I estimate. Yours should be very close, if not identical."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print hmm_weights['V','go',emit], hmm_weights['~','go',emit], hmm_weights['^','diddy',emit]\n",
      "print hmm_weights['V','V',trans], hmm_weights['~','V',trans]\n",
      "print hmm_weights[end_tag,'V',trans], hmm_weights[end_tag,'~',trans]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-4.66268727503 -13.2056617029 -1000.0\n",
        "-1.89367092996 -5.9130524537\n",
        "-4.30361454127 -3.06898273529\n"
       ]
      }
     ],
     "prompt_number": 397
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: here's the tag sequence and score for a modified version of our example sentence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "viterbiTagger([':))','we','can','can','fish',':-)'],hmm_feats,hmm_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 398,
       "text": [
        "(['E', 'O', 'V', 'V', 'N', 'E'], -47.970198717991266)"
       ]
      }
     ],
     "prompt_number": 398
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3d** (3 points): compute the predicted tag sequence and scores for the first three sentence in the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i,(words,_) in enumerate(conllSeqGenerator(trainfile)):\n",
      "    print i, viterbiTagger(words,hmm_feats,hmm_weights,alltags)\n",
      "    if i >= 2: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 3d** (2 points):\n",
      "- Run your HMM tagger on the dev data, using the code line below.\n",
      "- ** Sanity check**: I get 74.5% accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words, alltags : viterbiTagger(words,hmm_feats,hmm_weights,alltags)[0],'hmm')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}